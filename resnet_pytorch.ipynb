{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ä°s Cuda Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(path,num_img):\n",
    "    array = np.zeros([num_img,64*32])\n",
    "    i = 0\n",
    "    for image in os.listdir(path):\n",
    "        img_path = path + \"\\\\\" + image\n",
    "        if image.endswith(\".png\"):\n",
    "            img = Image.open(img_path, mode=\"r\")\n",
    "            data = np.asarray(img, dtype=\"uint8\")\n",
    "            data = data.flatten()\n",
    "            array[i,:] = data\n",
    "            i += 1\n",
    "        else:\n",
    "            pass\n",
    "    return array    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_path = r\"C:\\Users\\mustdo\\Desktop\\workspace\\Machine Learning\\deeplearning\\LSIFIR\\Classification\\Train\\neg\"\n",
    "train_pos_path = r\"C:\\Users\\mustdo\\Desktop\\workspace\\Machine Learning\\deeplearning\\LSIFIR\\Classification\\Train\\pos\"\n",
    "\n",
    "pos_nums = 10208\n",
    "neg_nums = 43390\n",
    "\n",
    "pos_train_array = read_images(train_pos_path,pos_nums)\n",
    "neg_train_array = read_images(train_neg_path,neg_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10208])\n",
      "torch.Size([43390])\n"
     ]
    }
   ],
   "source": [
    "x_pos_train_tensor = torch.from_numpy(pos_train_array)\n",
    "\n",
    "y_pos_train_tensor =torch.ones(pos_nums,dtype=torch.long)\n",
    "\n",
    "print(y_pos_train_tensor.size())\n",
    "\n",
    "x_neg_train_tensor = torch.from_numpy(neg_train_array)\n",
    "\n",
    "y_neg_train_tensor =torch.zeros(neg_nums,dtype=torch.long)\n",
    "\n",
    "print(y_neg_train_tensor.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([53598, 2048])\n",
      "torch.Size([53598])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.cat((x_neg_train_tensor,x_pos_train_tensor),0)\n",
    "y_train = torch.cat((y_neg_train_tensor,y_pos_train_tensor),0)\n",
    "print(x_train.size())\n",
    "\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg_path = r\"C:\\Users\\mustdo\\Desktop\\workspace\\Machine Learning\\deeplearning\\LSIFIR\\Classification\\Test\\neg\"\n",
    "test_pos_path = r\"C:\\Users\\mustdo\\Desktop\\workspace\\Machine Learning\\deeplearning\\LSIFIR\\Classification\\Test\\pos\"\n",
    "\n",
    "pos_test_nums = 5944\n",
    "neg_test_nums = 22050\n",
    "\n",
    "neg_test_array = read_images(test_neg_path,neg_test_nums)\n",
    "pos_test_array = read_images(test_pos_path,pos_test_nums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5944])\n",
      "torch.Size([22050])\n"
     ]
    }
   ],
   "source": [
    "x_pos_test_tensor = torch.from_numpy(pos_test_array)\n",
    "\n",
    "y_pos_test_tensor =torch.ones(pos_test_nums,dtype=torch.long)\n",
    "\n",
    "print(y_pos_test_tensor.size())\n",
    "\n",
    "x_neg_test_tensor = torch.from_numpy(neg_test_array)\n",
    "    \n",
    "y_neg_test_tensor =torch.zeros(neg_test_nums,dtype=torch.long)\n",
    "\n",
    "print(y_neg_test_tensor.size())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27994, 2048])\n",
      "torch.Size([27994])\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.cat((x_neg_test_tensor,x_pos_test_tensor),0)\n",
    "y_test = torch.cat((y_neg_test_tensor,y_pos_test_tensor),0)\n",
    "print(x_test.size())\n",
    "\n",
    "print(y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[13411,:].reshape(64,32),cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "num_classes = 2\n",
    "batch_size = 2\n",
    "lr = 0.0001\n",
    "\n",
    "from torch.utils import data\n",
    "    \n",
    "train = torch.utils.data.TensorDataset(x_train,y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = torch.utils.data.TensorDataset(x_test,y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep residual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_plane,out_plane,stride=1):\n",
    "    return nn.Conv2d(in_plane,out_plane,kernel_size=3,stride=stride,padding=1,bias= False)\n",
    "\n",
    "def conv1x1(in_plane,out_plane,stride=1):\n",
    "    return nn.Conv2d(in_plane, out_plane, kernel_size=1, stride=stride, bias= False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample= None):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.drop = nn.Dropout(0.9)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.drop(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers:list, num_classes = num_classes):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(1,64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64 ,layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128 ,layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256 ,layers[2], stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(256*block.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode=\"fan_out\",nonlinearity=\"relu\")\n",
    "            elif isinstance(m,nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride = 1):\n",
    "        downsample = None\n",
    "        \n",
    "        if stride != 1 or self.inplanes != planes*block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes*block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes*block.expansion))\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes*block.expansion\n",
    "        \n",
    "        for _ in range(1,blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(BasicBlock, [2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr =lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:=============> 0 0/26799\n",
      "Epoch:=============> 0 1000/26799\n",
      "Epoch:=============> 0 2000/26799\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "use_gpu = False\n",
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images = images.view(batch_size,1,64,32)\n",
    "        images = images.float()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%1000 == 0:\n",
    "            print(\"Epoch:=============> {} {}/{}\".format(epoch,i,total_step))\n",
    "            \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad:\n",
    "        for data in train_loader:\n",
    "            images,labels = data\n",
    "            images = images.view(batch_size,1,64,32)\n",
    "            images = images.float()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _,predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct = (predicted==labels).sum().item()\n",
    "    print(\"Train accuracy : %d  %%\"%(100*correct/total))\n",
    "    train_acc.append(100*correct/total)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad:\n",
    "        for data in test_loader:\n",
    "            images,labels = data\n",
    "            images = images.view(batch_size,1,64,32)\n",
    "            images = images.float()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _,predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct = (predicted==labels).sum().item()\n",
    "    print(\"Test accuracy : %d  %%\"%(100*correct/total))\n",
    "    test_acc.append(100*correct/total)\n",
    "    loss_list.append(loss.item())\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
